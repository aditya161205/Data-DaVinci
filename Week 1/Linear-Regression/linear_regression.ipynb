{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION\n",
    "In this assignment we try to model the 'Estimated Price' as a linear relation of the other elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSES IN PYTHON\n",
    "Although this might look scary to implement, go about it one function at a time.  \n",
    "Using classes help with keeping track of multiple models and makes your overall code much tidier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self) -> None:\n",
    "       \n",
    "        self.weights: np.ndarray | None = None\n",
    "        self.bias: float | None = None\n",
    "    \n",
    "\n",
    "\n",
    "    ### TODO 1\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        Y_pred = np.ndarray( np.dot(X,self.weights) + self.bias)\n",
    "        print(Y_pred.shape)\n",
    "        return Y_pred\n",
    "\n",
    "\n",
    "\n",
    "    ### TODO 2 \n",
    "    def __loss(self, X: np.ndarray, y: np.ndarray, norm: int) -> tuple:\n",
    "       \n",
    "        predicted = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "        \n",
    "        errors = np.abs(predicted - y) ** norm\n",
    "        loss = (1 / X.shape[0]) * np.sum(errors)\n",
    "\n",
    "        \n",
    "        gradient_base = norm * np.sign(predicted - y) * (np.abs(predicted - y) ** (norm - 1))\n",
    "\n",
    "        dw = (1 / X.shape[0]) * np.dot(X.T, gradient_base)\n",
    "        db = (1 / X.shape[0]) * np.sum(gradient_base)\n",
    "\n",
    "        return loss, dw, db\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 500, learning_rate: float = 0.01, norm: int = 2, threshold: float = 0.0001) -> None:\n",
    "   \n",
    "\n",
    "        self.weights = np.random.randn(X.shape[1])\n",
    "        self.bias = 0\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for _ in range(epochs):\n",
    "           \n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            current_loss, dw, db = self.__loss(X, y_pred, norm)\n",
    "\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "\n",
    "            if abs(current_loss - prev_loss) < threshold:\n",
    "                break\n",
    "            \n",
    "            prev_loss = current_loss  \n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Converting Data\n",
    "Some features in a dataset are not of numerical type and are either categorical or boolean.  \n",
    "To get past this, we convert the columns by using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "df = pd.read_csv('linear_data.csv')\n",
    "\n",
    "### TODO 4\n",
    "df_onehot=pd.get_dummies(df)\n",
    "\n",
    "X=df_onehot.drop(columns='Estimated Price')\n",
    "y=df_onehot['Estimated Price']\n",
    "\n",
    "X = X.to_numpy() \n",
    "y = y.to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-train split\n",
    "Overfitting is one of the biggest problems in machine learning. Overfitting occurs when the model is trained to be very accurate on the given dataset but performs very poorly on a different but similar dataset.\n",
    "To check for overfitting, we split our dataset into test and train sets and check the accuracy/loss of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Standardization\n",
    "Since some features might have much higher values than the others, for weights of similar magnitude, the model will mainly focus only on features with large values.  \n",
    "To overcome this, we standardize each feature using Z-Score Standardization so that all features are treated equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score standardization\n",
    "### TODO 5\n",
    "def z_score(X: np.ndarray) -> tuple:\n",
    "    '''\n",
    "    The Z-Score scales data such that its mean is 0 and standard deviation is 1\n",
    "    z-score for a value x in the dataset is (x - mean) / std_dev\n",
    "    (z-score normalization is done over a feature and NOT an entry)\n",
    "    Return the z-score value of all the elements in the set along with the mean and standard deviation of the original set\n",
    "    '''\n",
    "\n",
    "    x_mean = np.mean(X,axis=0)\n",
    "\n",
    "    x_std = np.std(X,mean=0)\n",
    "\n",
    "    x = (X-x_mean)/x_std\n",
    "    return x, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "x_train, x_mean, x_std = z_score(X_train)\n",
    "x_test = (X_test - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "maximum supported dimension for an ndarray is currently 64, found 800",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean((y_pred \u001b[38;5;241m-\u001b[39m y_test) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[31], line 48\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, epochs, learning_rate, norm, threshold)\u001b[0m\n\u001b[1;32m     44\u001b[0m prev_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Predict the target values using the current weights and bias\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Calculate loss and gradients based on predictions (y_pred)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     current_loss, dw, db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__loss(X, y_pred, norm)\n",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m, in \u001b[0;36mLinearRegression.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 12\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y_pred\n",
      "\u001b[0;31mValueError\u001b[0m: maximum supported dimension for an ndarray is currently 64, found 800"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train, epochs=500, learning_rate=0.01, norm=2, threshold=0.0001)\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"MSE loss: \", np.mean((y_pred - y_test) ** 2))\n",
    "\n",
    "indices = np.arange(len(y_test))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(indices, y_test, label='True Values', color='blue', marker='o')\n",
    "plt.plot(indices, y_pred, label='Predicted Values', color='red', marker='x')\n",
    "\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Values')\n",
    "plt.title('True vs Predicted Values')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
