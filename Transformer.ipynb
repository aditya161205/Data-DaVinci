{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDsDH/6kL4uj3PLovD7NX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya161205/Data-DaVinci/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XZmNmjfv9z19"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "coding multihead attention"
      ],
      "metadata": {
        "id": "OW359DXk-niJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_mod,num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.d_mod=d_mod #this is embeding size\n",
        "    self.num_heads=num_heads\n",
        "    self.d_k=d_mod//num_heads\n",
        "\n",
        "    self.W_k=nn.linear(self.d_mod,self.d_mod)\n",
        "    self.W_v=nn.linear(self.d_mod,self.d_mod)\n",
        "    self.W_q=nn.linear(self.d_mod,self.d_mod)\n",
        "    self.W_o=nn.linear(self.d_mod,self.d_mod)\n",
        "\n",
        "  def ScaledDotProduct(self,Q,K,V,mask=False):\n",
        "    attn=torch.matmul(Q,K.transpose(-2,-1))/ math.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        #if the value at mask is 0 mean that we have to ignore it so we assign -inf which is squeezed to 0 by softmax\n",
        "    attn_prob=torch.softmax(attn,dim=-1)\n",
        "    attn_val=torch.matmul(attn_prob,V)\n",
        "    return attn_val\n",
        "\n",
        "  def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "      batch_size, _, seq_length, d_k = x.size()\n",
        "      return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self,Q,K,V,mask):\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    attn_val=self.ScaledDotProduct(Q,K,V,mask)\n",
        "    output=self.W_o(self.combine_heads(attn_val))\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "V03E7WL7-c4D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_mod, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.l1=nn.linear(d_mod,d_ff)\n",
        "        self.l2=nn.linear(d_ff,d_mod)\n",
        "        self.activation=nn.ReLU()\n",
        "    def forward(self,x):\n",
        "      x=self.l1(x)\n",
        "      x=self.activation(x)\n",
        "      x=self.l2(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "kQ150hCw_ZO2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "taking the code for positional embeding from medium...sorry"
      ],
      "metadata": {
        "id": "a-tfPVTjMgT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_mod, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_mod)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_mod, 2).float() * -(math.log(10000.0) / d_mod))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "x-XCTkGwKAxV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_mod, num_heads, d_ff, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention=MultiHeadAttention(d_mod,num_heads)\n",
        "    self.feed_forward=PositionWiseFeedForward(d_mod,d_ff)\n",
        "    self.norm1=nn.LayerNorm(d_mod)\n",
        "    self.dropout1=nn.Dropout(dropout)\n",
        "    self.norm2=nn.LayerNorm(d_mod)\n",
        "    self.dropout2=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    attn=self.attention(x,x,x,mask)\n",
        "    x=self.norm1(x+self.dropout1(attn))\n",
        "    ff_out=self.feed_forward(x)\n",
        "    x = self.norm2(x + self.dropout2(ff_out))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Ze2my0hYMwo5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,d_mod,num_heads,d_ff,dropout):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.self_attn=MultiHeadAttention(d_mod,num_heads)\n",
        "    self.cross_attn=MultiHeadAttention(d_mod,num_heads)\n",
        "    self.feen_forward=PositionWiseFeedForward(d_mod,d_ff)\n",
        "    self.norm1=nn.LayerNorm(d_mod)\n",
        "    self.dropout1=nn.Dropout(dropout)\n",
        "    self.norm2=nn.LayerNorm(d_mod)\n",
        "    self.dropout2=nn.Dropout(dropout)\n",
        "    self.norm3=nn.LayerNorm(d_mod)\n",
        "    self.dropout3=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self,x,enc_output,src_mask,trgt_mask):\n",
        "    self_attn_score=self.self_attn(x,x,x,trgt_mask)\n",
        "    x=self.norm1(x+self.dropout1(self_attn_score))\n",
        "    cross_attn_score=self.cross_attn(enc_output,enc_output,x,trgt_mask)\n",
        "    x=self.norm2(x+self.dropout2(cross_attn_score))\n",
        "    feed_output=self.feen_forward(x)\n",
        "    x=self.norm3(x+self.dropout3(feed_output))\n",
        "    return x"
      ],
      "metadata": {
        "id": "DF5lLU17RJm1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SEQo8FBrub1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}